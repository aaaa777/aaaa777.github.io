---
layout: post
title: "LLMに触れてみる"
description: desc
hero_image: /img/screenshot-2023-11-20 215143.png
image: /img/screenshot-2023-11-20 215143.png
hero_height: is-large
hero_darken: true
tags:
 - blog
 - dialy
#  - hidden
---

ここ数日自分の中でAIブームが来てます。

ChatGPTやBardやGithub copilot等のAIサービスは日常的に使うようになりました。
とっても便利なんですが、開発者たるもの自分が使うサービスはせめて自分で実装出来るようにならないとなと思っているので、AIをちゃんと扱ったことのない私は少しの後ろめたさを感じていました。

でもGithub Copilot Workspaceが発表され、いよいよエンジニア人生の危機を感じた私はとりあえずllama2を日本語ファインチューニングしたElyzaというモデルを動かしてみることにしました。
Colabで動かしてみたのですが、最小の7bモデルの読み込みで大体15GB程のVRAMを消費していて生成の途中でこけることもしばしば。
自分のGPUは1050Ti 4GBなので載るわけもなく、かといってお金もないのでどうにかする方法を考えてみました。

## 問題点

問題は必要なVRAMが多すぎることです。
解決方法は高いグラボでVRAMを増やすか、モデルを小さくしてVRAMを減らすかです。
グラボは買えないのでモデルを小さくする方法を考えます。

ChatGPTなどの元になったTransformerのモデルで一番重要なのはAttention層という層です。
簡単に言えば読み込んだ文章の次の単語を予測するのに使われる層で、例えば「今日の天気は」と来たら「晴れ」とか「雨」とかが来るかなと予測します。
重要なのは「天気」という単語に注目して「晴れ」とか「雨」のような単語が活性化する仕組みで、このおかげで人間のような単語の連想が出来ます。
このとき連想をするために入力文章と語彙の行列を作って計算するため、文章か語彙が増加すると行列のサイズが大きくなりVRAMの消費量が増えます。

ここで一つの仮説を立てておきます。
GoogleやOpenAIの賢い人たちが頑張ってTransformerモデルで容量を削った結果が今のモデルサイズなので、GPT3.5くらいの性能で受け答えが出来るモデルはこのサイズが限界なはずです。
であるならば私は別のアプローチを考える必要があるのではないかと考えました。

## 改善の仮説

次の単語を予測するのに私たちは全ての単語を連想しているわけではありません。
先ほどの「天気」の例でいうと、「晴れ」や「雨」は連想できますが「トマト」とか「猫」とかは連想できません。
なので連想できる単語の結びつきが弱いところで切り離せば、モデルを分割して都度読み込むことが出来るのではないでしょうか。

遅くなっても構わないところはネットワーク越しにモデルを分割してもいいかもしれません。
後から来たデータを入力に追加できるような仕組みとかあれば面白いかもしれません。

とりあえず色々な会話データを投入して、まとまって活性化しやすい領域の範囲を調べて語彙が分割できるかどうか調べてみたいと思います。

## 注

分かりやすい表現にするために「語彙」という単語を使っていますが、正確にはTokenizerで分割された単語のことです。
また理解が間違っている可能性もあります。