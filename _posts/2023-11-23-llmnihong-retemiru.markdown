---
layout: post
title: "LLMに触れてみる"
description: desc
hero_image: /img/screenshot-2023-11-20 215143.png
image: /img/screenshot-2023-11-20 215143.png
hero_height: is-large
hero_darken: true
tags:
 - blog
 - dialy
#  - hidden
---

ここ数日自分の中でAIブームが来てます。

ChatGPTやBardやGithub copilot等のAIサービスは日常的に使うようになりました。
とっても便利なんですが、開発者たるもの自分が使うサービスはせめて自分で実装出来るようにならないとなと思っているので、AIをちゃんと扱ったことのない私は少しの後ろめたさを感じていました。

でもGithub Copilot Workspaceが発表され、いよいよエンジニア人生の危機を感じた私はとりあえずllama2を日本語ファインチューニングしたElyzaというモデルを動かしてみることにしました。
Colabで動かしてみたのですが、最小の7bモデルの読み込みで大体15GB程のVRAMを消費していて生成の途中でこけることもしばしば。
自分のGPUは1050Ti 4GBなので載るわけもなく、かといってお金もないのでどうにかする方法を考えてみました。

## 問題点

問題は必要なVRAMが多すぎることです。
解決方法は高いグラボでVRAMを増やすか、モデルを小さくしてVRAMを減らすかです。
グラボは買えないのでモデルを小さくする方法を考えます。

ChatGPTなどの元になったTransformerのモデルで一番重要なのはAttention層という層です。
簡単に言えば読み込んだ文章の次の単語を予測するのに使われる層で、例えば「今日の天気は」と来たら「晴れ」とか「雨」とかが来るかなと予測します。
重要なのは「天気」という単語に注目して「晴れ」とか「雨」のような単語が活性化する仕組みで、このおかげで人間のような単語の連想が出来ます。
このとき連想をするために入力文章と語彙の行列を作って計算するため、文章か語彙が増加すると行列のサイズが大きくなりVRAMの消費量が増えます。

##### 行列の雑なイメージ
```
It      0       0       0       0       0.001   0     ...
Will    0       0.02    0.01    0       0       0.01  ...
        <eof>   Cold    Rain    Cat     Tomato  Sunny ...
```

ここで一つの仮説を立てておきます。
GoogleやOpenAIの賢い人たちが頑張ってTransformerモデルで容量を削った結果が今のモデルサイズなので、GPT3.5くらいの性能で受け答えが出来るモデルはこのサイズが限界なはずです。
であるならば別のアプローチを考えるべきです。

## 改善の仮説

次の単語を予測するのに私たちは全ての単語を連想しているわけではありません。
先ほどの「天気」の例でいうと、「晴れ」や「雨」は連想できますが「トマト」とか「猫」とかは連想できません。
連想できる単語の結びつきが弱いところで切り離せば、モデルを分割して必要な時に都度読み込むことが出来るのではないでしょうか。

遅くなっても構わないところはネットワーク越しにモデルを分割してもいいかもしれません。
後から来たデータを入力に追加できるような仕組みとか作ってみたいですね。

とりあえず色々な会話データを投入して、まとまって活性化しやすい領域の範囲を調べて語彙が分割できるかどうか調べてみたいと思います。


## その他雑記

分散計算の遅延を考慮したリアルタイム性の高いLLMって存在するんでしょうか。
重要でない領域の計算を待たずに先に進めるような仕組みがあれば分割は必要ないかもしれません。
私は数学者よりもエンジニアなので、LLMを改善するよりコンピュータの世界に近づけることに思考のリソースを割いた方がいいような気がしています。

あと自然言語のタスクに対する解決法はいくつもあるけど、現在のLLMは最適解ではないと思っています。
汎用性のあるLLMよりモデル化されたタスクをこなす機械の方が効率が良いですし、環境にも良いはずです。
私は分割統治法のようにタスクを小さくしていって、LLMの領域を小さくしていければ良いなと思います。

## 注

分かりやすい表現にするために「語彙」という単語を使っていますが、正確にはTokenizerで分割された単語のことです。
入力もEncoderによってベクトル化されているので、直接単語を扱っているわけではありません。
また理解が間違っている可能性もあることをご承知おきください。